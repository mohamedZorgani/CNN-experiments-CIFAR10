{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:35.120415Z",
     "iopub.status.busy": "2024-10-29T21:12:35.120005Z",
     "iopub.status.idle": "2024-10-29T21:12:40.193850Z",
     "shell.execute_reply": "2024-10-29T21:12:40.193068Z",
     "shell.execute_reply.started": "2024-10-29T21:12:35.120375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision.models import vgg11_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:40.195980Z",
     "iopub.status.busy": "2024-10-29T21:12:40.195556Z",
     "iopub.status.idle": "2024-10-29T21:12:40.199991Z",
     "shell.execute_reply": "2024-10-29T21:12:40.199112Z",
     "shell.execute_reply.started": "2024-10-29T21:12:40.195946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:40.201479Z",
     "iopub.status.busy": "2024-10-29T21:12:40.201150Z",
     "iopub.status.idle": "2024-10-29T21:12:40.216201Z",
     "shell.execute_reply": "2024-10-29T21:12:40.215333Z",
     "shell.execute_reply.started": "2024-10-29T21:12:40.201448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(144),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)), \n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:40.218915Z",
     "iopub.status.busy": "2024-10-29T21:12:40.218581Z",
     "iopub.status.idle": "2024-10-29T21:12:45.587148Z",
     "shell.execute_reply": "2024-10-29T21:12:45.586348Z",
     "shell.execute_reply.started": "2024-10-29T21:12:40.218861Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 84215324.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = DataLoader(dataset=train_set , batch_size=256 ,shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:45.588575Z",
     "iopub.status.busy": "2024-10-29T21:12:45.588258Z",
     "iopub.status.idle": "2024-10-29T21:12:45.625860Z",
     "shell.execute_reply": "2024-10-29T21:12:45.625027Z",
     "shell.execute_reply.started": "2024-10-29T21:12:45.588541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 50000\n",
      "Validation dataset size: 5000\n",
      "Test dataset size: 5000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "val_size = 5000  \n",
    "test_size = len(test_set) - val_size\n",
    "\n",
    "val_set, test_set = random_split(test_set, [val_size, test_size])\n",
    "\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_set)}\")\n",
    "print(f\"Validation dataset size: {len(val_set)}\")\n",
    "print(f\"Test dataset size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:45.627861Z",
     "iopub.status.busy": "2024-10-29T21:12:45.627126Z",
     "iopub.status.idle": "2024-10-29T21:12:45.720730Z",
     "shell.execute_reply": "2024-10-29T21:12:45.719858Z",
     "shell.execute_reply.started": "2024-10-29T21:12:45.627817Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:45.722437Z",
     "iopub.status.busy": "2024-10-29T21:12:45.721955Z",
     "iopub.status.idle": "2024-10-29T21:12:49.666970Z",
     "shell.execute_reply": "2024-10-29T21:12:49.665886Z",
     "shell.execute_reply.started": "2024-10-29T21:12:45.722394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n",
      "100%|██████████| 507M/507M [00:02<00:00, 222MB/s]  \n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained vgg11_bn\n",
    "model = vgg11_bn(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:49.668545Z",
     "iopub.status.busy": "2024-10-29T21:12:49.668207Z",
     "iopub.status.idle": "2024-10-29T21:12:49.675613Z",
     "shell.execute_reply": "2024-10-29T21:12:49.674632Z",
     "shell.execute_reply.started": "2024-10-29T21:12:49.668510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:49.677538Z",
     "iopub.status.busy": "2024-10-29T21:12:49.677156Z",
     "iopub.status.idle": "2024-10-29T21:12:49.703786Z",
     "shell.execute_reply": "2024-10-29T21:12:49.703035Z",
     "shell.execute_reply.started": "2024-10-29T21:12:49.677495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features = nn.Sequential(*list(model.features.children())[:28])\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=(3, 3)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(4608, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, 512),  \n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    features,\n",
    "    classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:12:49.755152Z",
     "iopub.status.busy": "2024-10-29T21:12:49.754834Z",
     "iopub.status.idle": "2024-10-29T21:13:02.809749Z",
     "shell.execute_reply": "2024-10-29T21:13:02.808777Z",
     "shell.execute_reply.started": "2024-10-29T21:12:49.755118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(256,3,128,128)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:13:02.811440Z",
     "iopub.status.busy": "2024-10-29T21:13:02.811058Z",
     "iopub.status.idle": "2024-10-29T21:13:02.845378Z",
     "shell.execute_reply": "2024-10-29T21:13:02.844549Z",
     "shell.execute_reply.started": "2024-10-29T21:13:02.811397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:13:02.868335Z",
     "iopub.status.busy": "2024-10-29T21:13:02.868080Z",
     "iopub.status.idle": "2024-10-29T21:13:03.106772Z",
     "shell.execute_reply": "2024-10-29T21:13:03.105765Z",
     "shell.execute_reply.started": "2024-10-29T21:13:02.868301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9 ,weight_decay = 5e-4 )\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T21:13:03.108656Z",
     "iopub.status.busy": "2024-10-29T21:13:03.108262Z",
     "iopub.status.idle": "2024-10-29T22:13:01.672013Z",
     "shell.execute_reply": "2024-10-29T22:13:01.670489Z",
     "shell.execute_reply.started": "2024-10-29T21:13:03.108613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/tmp/ipykernel_30/2898930494.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # No need to compute gradients during validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400]\n",
      "Train Loss: 1.0832 | Train Accuracy: 76.56%\n",
      "Val Loss: 0.7662 | Val Accuracy: 89.82%\n",
      "Epoch [2/400]\n",
      "Train Loss: 0.7940 | Train Accuracy: 89.77%\n",
      "Val Loss: 0.6990 | Val Accuracy: 92.34%\n",
      "Epoch [3/400]\n",
      "Train Loss: 0.7224 | Train Accuracy: 92.66%\n",
      "Val Loss: 0.6777 | Val Accuracy: 92.56%\n",
      "Epoch [4/400]\n",
      "Train Loss: 0.6807 | Train Accuracy: 94.11%\n",
      "Val Loss: 0.6593 | Val Accuracy: 93.40%\n",
      "Epoch [5/400]\n",
      "Train Loss: 0.6478 | Train Accuracy: 95.44%\n",
      "Val Loss: 0.6572 | Val Accuracy: 93.60%\n",
      "Epoch [6/400]\n",
      "Train Loss: 0.6209 | Train Accuracy: 96.56%\n",
      "Val Loss: 0.6517 | Val Accuracy: 93.70%\n",
      "Epoch [7/400]\n",
      "Train Loss: 0.6048 | Train Accuracy: 97.19%\n",
      "Val Loss: 0.6597 | Val Accuracy: 93.62%\n",
      "Epoch [8/400]\n",
      "Train Loss: 0.5875 | Train Accuracy: 97.84%\n",
      "Val Loss: 0.6665 | Val Accuracy: 93.56%\n",
      "Epoch [9/400]\n",
      "Train Loss: 0.5773 | Train Accuracy: 98.20%\n",
      "Val Loss: 0.6582 | Val Accuracy: 93.66%\n",
      "Epoch [10/400]\n",
      "Train Loss: 0.5698 | Train Accuracy: 98.47%\n",
      "Val Loss: 0.6601 | Val Accuracy: 93.40%\n",
      "Epoch [11/400]\n",
      "Train Loss: 0.5551 | Train Accuracy: 99.15%\n",
      "Val Loss: 0.6406 | Val Accuracy: 94.14%\n",
      "Epoch [12/400]\n",
      "Train Loss: 0.5517 | Train Accuracy: 99.28%\n",
      "Val Loss: 0.6370 | Val Accuracy: 94.32%\n",
      "Epoch [13/400]\n",
      "Train Loss: 0.5496 | Train Accuracy: 99.31%\n",
      "Val Loss: 0.6379 | Val Accuracy: 94.20%\n",
      "Epoch [14/400]\n",
      "Train Loss: 0.5482 | Train Accuracy: 99.35%\n",
      "Val Loss: 0.6362 | Val Accuracy: 94.42%\n",
      "Epoch [15/400]\n",
      "Train Loss: 0.5457 | Train Accuracy: 99.47%\n",
      "Val Loss: 0.6391 | Val Accuracy: 94.06%\n",
      "Epoch [16/400]\n",
      "Train Loss: 0.5459 | Train Accuracy: 99.45%\n",
      "Val Loss: 0.6341 | Val Accuracy: 94.26%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m correct_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1280\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1280\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:968\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    966\u001b[0m     _log_api_usage_once(adjust_hue)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:117\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    113\u001b[0m np_h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hue_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    115\u001b[0m h \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np_h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHSV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:1146\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     dither \u001b[38;5;241m=\u001b[39m Dither\u001b[38;5;241m.\u001b[39mFLOYDSTEINBERG\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1146\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 400\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        running_train_loss += loss.item() \n",
    "        \n",
    "        # Track training accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_loss = running_train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  \n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.cuda.amp.autocast(): \n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  \n",
    "            loss = criterion(outputs, labels)  \n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_loss = running_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    scheduler.step()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T22:13:13.621444Z",
     "iopub.status.busy": "2024-10-29T22:13:13.620759Z",
     "iopub.status.idle": "2024-10-29T22:13:13.627435Z",
     "shell.execute_reply": "2024-10-29T22:13:13.626542Z",
     "shell.execute_reply.started": "2024-10-29T22:13:13.621402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device = device)\n",
    "            y = y.to(device = device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    #model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T22:13:17.889747Z",
     "iopub.status.busy": "2024-10-29T22:13:17.889357Z",
     "iopub.status.idle": "2024-10-29T22:13:22.231864Z",
     "shell.execute_reply": "2024-10-29T22:13:22.230908Z",
     "shell.execute_reply.started": "2024-10-29T22:13:17.889709Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9412, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-29T22:13:56.829682Z",
     "iopub.status.busy": "2024-10-29T22:13:56.828715Z",
     "iopub.status.idle": "2024-10-29T22:13:56.961691Z",
     "shell.execute_reply": "2024-10-29T22:13:56.960682Z",
     "shell.execute_reply.started": "2024-10-29T22:13:56.829639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "        torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        }, \"model_checkpoint09412.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
